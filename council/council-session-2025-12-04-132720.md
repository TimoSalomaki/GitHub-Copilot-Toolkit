# AI Council Session

**Created:** 2025-12-04 13:27:20  
**Topic:** How should a visualization engine be implemented for a web-based music streaming platform using Next.js? Aim for a modular, extensible, and well-architected outcome.  
**Status:** âœ… CONSENSUS REACHED

## Council Members
- [x] Claude (Claude Sonnet 4.5)
- [x] GPT (GPT-5)  
- [x] Gemini (Gemini 3 Pro)

## Consensus Tracking
**Proposals on the table:** Unified Worker-Centric Tiered Architecture  
**Agreement status:** âœ… CONSENSUS REACHED

---

## Deliberation Record

---

### ðŸŸ£ Claude - Round 1
**Timestamp:** 2025-12-04 13:27:20

#### My Thinking

A visualization engine for a music streaming platform is a critical component that sits at the intersection of performance, user experience, and technical architecture. I need to consider several key dimensions:

1. **Rendering Technology**: WebGL/WebGPU vs Canvas 2D vs SVG
2. **Audio Analysis Pipeline**: How we extract frequency, amplitude, and timing data
3. **Modularity**: Plugin architecture for different visualizations
4. **Performance**: 60fps rendering without impacting playback
5. **Next.js Integration**: SSR considerations, client-side hydration, edge runtime compatibility

#### Key Points

**Architecture Layers:**
- **Audio Analysis Layer**: Web Audio API for real-time FFT analysis
  - AnalyserNode for frequency/time domain data
  - Configurable FFT size (2048-8192 bins)
  - Decoupled from rendering for performance

- **Visualization Core Engine**:
  - Abstract base class/interface for all visualizers
  - Standardized lifecycle: `init()`, `update(audioData)`, `render()`, `destroy()`
  - Resource management (cleanup, memory pooling)
  - Frame timing and throttling system

- **Renderer Abstraction**:
  - Support multiple backends (WebGL, Canvas2D, WebGPU when available)
  - Capability detection and graceful degradation
  - Shared utilities (color palettes, easing functions, particle systems)

- **Plugin System**:
  - Dynamic imports for code-splitting per visualization
  - Manifest-based registration
  - Hot-swappable at runtime
  - Preset/configuration schema (JSON-serializable)

**Next.js Specific Considerations:**
- Client-only components (`'use client'` directive) for audio/visualization
- Dynamic imports with `ssr: false` for heavy visualization libraries
- Lazy loading of visualizations on-demand
- Route prefetching considerations for smooth transitions
- Server Components for visualization picker/settings UI

**State Management:**
- Separate audio state from visualization state
- Consider Zustand/Jotai for lightweight reactive state
- Avoid prop drilling through context or atomic state
- Persist user visualization preferences (localStorage + database sync)

#### Questions for the Council

1. **WebGL vs Canvas2D**: Should we prioritize WebGL for all visualizations, or maintain a dual-path approach for simpler ones? WebGL has overhead but much better performance for complex effects.

2. **Worker Threads**: Should audio analysis run in a Web Worker to prevent main thread blocking, or is the Audio Worklet API sufficient for our needs?

3. **Preset System**: How granular should user customization be? Simple presets vs. full parameter exposure (color schemes, sensitivity, smoothing factors)?

4. **Mobile Performance**: Should we have a separate, simplified rendering path for mobile devices, or rely on capability detection to scale down effects?

5. **Integration Point**: Should visualizations be a separate micro-frontend or tightly integrated into the main Next.js app? Consider bundle size implications.

#### Position/Proposal

I propose a **layered architecture** with the following structure:

```
/lib/visualization/
  /core/
    - VisualizationEngine.ts (orchestrator)
    - AudioAnalyzer.ts (Web Audio API wrapper)
    - Renderer.ts (abstract base)
  /renderers/
    - WebGLRenderer.ts
    - Canvas2DRenderer.ts
  /visualizers/
    - BaseVisualizer.ts (abstract)
    - /presets/
      - SpectrumBars.ts
      - Waveform.ts
      - Particles.ts
      - CircularSpectrum.ts
  /utils/
    - ColorPalettes.ts
    - EasingFunctions.ts
    - PerformanceMonitor.ts

/components/
  - VisualizationCanvas.tsx (React wrapper)
  - VisualizationControls.tsx (settings UI)
  - VisualizationPicker.tsx (preset selector)
```

**Technology Stack:**
- TypeScript for type safety
- Web Audio API for analysis
- WebGL2 as primary renderer (Canvas2D fallback)
- React hooks for component integration
- Dynamic imports for code-splitting

I lean toward WebGL as the primary rendering technology with a Canvas2D fallback, and I believe audio analysis should stay on the main thread initially (optimize if profiling shows issues). Let's discuss these architectural decisions.

---

### ðŸ”µ Gemini - Round 1
**Timestamp:** 2025-12-04 13:30:00

#### Analysis of Previous Discussion
Claude has established a robust structural baseline. The layered architecture (Analysis -> Core -> Renderer) is sound. However, I believe the proposal is too conservative regarding threading and rendering technology for a 2025 context. Relying on the main thread for both React's reconciliation and a 60fps visualization loop is a recipe for dropped frames during UI interactions (like navigating playlists).

#### My Thinking
<think>
1.  **Threading Model**: Claude suggests keeping analysis on the main thread. In a Next.js app, the main thread is busy. If we want "console-quality" visualizations, we must decouple. The `OffscreenCanvas` API allows us to run the entire rendering loop in a Web Worker. This is critical.
2.  **WebGPU**: It's 2025. WebGL2 is legacy. WebGPU allows compute shaders, which enables particle systems with millions of points (boids, fluid sims) that react to music. This is the differentiator. We should design for WebGPU first, with a WebGL fallback layer.
3.  **Data Transfer**: If we move rendering to a Worker, how do we get Audio Data there? `AnalyserNode` lives in the Audio Context (usually main thread or audio thread). We can't pass the node to a worker. We need a `SharedArrayBuffer` to sync frequency data between the main thread (where the `<audio>` element likely lives) and the Render Worker without serialization overhead.
4.  **Streaming Reality**: We are building for a *streaming* platform. We will likely use `MediaElementSourceNode`. We must address CORS. If the CDN doesn't send `Access-Control-Allow-Origin`, the `AnalyserNode` will silently output zeroes. This is a common "gotcha".
</think>

#### Key Points
- **OffscreenCanvas & Workers**: I propose moving the *entire* visualization loop (update + render) to a Web Worker using `OffscreenCanvas`. This completely decouples the visualization framerate from React's UI updates.
- **SharedArrayBuffer**: Use a `SharedArrayBuffer` ring buffer to pass frequency/time-domain data from the Main Thread (Audio Context) to the Render Worker. This eliminates postMessage overhead for high-frequency data.
- **WebGPU First**: The architecture should define a `GPURenderer` interface. WebGPU's Compute Shaders are essential for high-end music visualization (e.g., GPU-based FFT, fluid dynamics).
- **CORS & Security**: We must explicitly handle `crossOrigin="anonymous"` on the audio element and ensure the CDN configuration supports it, or the visualization will fail silently due to tainted media restrictions.

#### Questions for the Council
1.  **Fallback Complexity**: If we go "WebGPU First", is the maintenance burden of a WebGL fallback too high? Should we use an abstraction library like `wgpu-matrix` or a higher-level engine (Three.js/Babylon.js) that handles this abstraction for us, or build raw for performance?
2.  **Audio Source**: Are we assuming a standard `<audio>` tag or a custom AudioWorklet-based playback engine? This impacts how we tap into the stream.

#### Position/Proposal
I propose modifying the architecture to be **Worker-Centric**:

**Revised Architecture:**
```
/lib/visualization/
  /worker/
    - RenderWorker.ts (The "Game Loop" lives here)
    - MessageHandler.ts (Control signals from UI)
  /core/
    - AudioBridge.ts (Main thread: Taps AudioContext -> writes to SharedArrayBuffer)
    - SyncBuffer.ts (Shared memory structure)
  /renderers/
    - WebGPURenderer.ts (Primary: Compute + Render pipelines)
    - WebGLFallbackRenderer.ts (Secondary)
```

**Next.js Integration:**
The `VisualizationCanvas.tsx` component effectively becomes a "portal" that just hands off a DOM Canvas control to the worker and manages the `Worker` lifecycle.

I vote to **REJECT** the "Main Thread" approach for rendering and **ACCEPT** a Worker-based approach.

---

### ðŸŸ¢ GPT - Round 1
**Timestamp:** 2025-12-04 13:33:30

#### Analysis of Previous Discussion
Claude proposes a clear, layered baseline centered on WebGL with Canvas2D fallback and main-thread analysis; Gemini advances a worker-centric, WebGPU-first design with SharedArrayBuffer for audio data. Both are directionally solid. The synthesis should balance pragmatic rollout (broad device support, maintainability) with a performance ceiling high enough for premium visualizations.

#### My Thinking
<think>
1. Compatibility matters for a streaming platform with heterogeneous devices. WebGPU-first is ideal, but we need a graceful fallback and runtime capability detection.
2. Decoupling rendering from the main thread is key. OffscreenCanvas + Worker reduces contention with Next.js UI and routing.
3. Audio analysis placement: AudioWorklet provides stable timing on an audio thread; use it to compute FFT windows and write into SharedArrayBuffer to the render worker.
4. Developer ergonomics: A small renderer abstraction plus preset schema keeps adding new visualizers simple and testable.
5. Observability: Built-in performance telemetry (frame time, dropped frames, buffer lag) informs dynamic quality scaling.
</think>

#### Key Points
- **Tiered Capability Model:** Detect `navigator.gpu` â†’ use WebGPU; else WebGL2; else Canvas2D minimal mode. Centralize detection in a `Capabilities` module.
- **Worker-Centric Loop:** Move update/render to a dedicated worker via `OffscreenCanvas`. UI communicates via command messages; audio data via `SharedArrayBuffer`.
- **AudioWorklet Bridge:** Use `AudioWorkletProcessor` to compute FFT/spectral features and write into shared memory; fallback to main-thread `AnalyserNode` if Worklet unsupported.
- **Plugin Visualizers:** Define `Visualizer` interface with lifecycle hooks and a declarative `params` schema. Load via dynamic import; versioned manifests for presets.
- **Quality Scaling:** Implement adaptive quality: resolution scale, particle count, post-processing toggles based on measured frame budget and device class.
- **Next.js Integration:** Client components for canvas/controls; `ssr: false` dynamic import for heavy libs; settings UI as Server Components; persist preferences via localStorage + user profile.
- **CORS Hygiene:** Require `crossOrigin="anonymous"` and validate CDN headers to avoid tainted media blocking visualization.

#### Questions for the Council
- What minimum device class do we target for premium effects (desktop only), and what is the acceptable mobile fallback quality?
- Should we adopt a small helper library (e.g., regl or wgpu-friendly math) to reduce boilerplate without locking into a heavy engine?
- How do we version visualizer presets to ensure backward compatibility when parameters evolve?

#### Position/Proposal
Adopt a **hybrid, worker-centric, tiered-capability architecture**:

```
/lib/visualization/
  /capabilities/
    - detect.ts
  /bridge/
    - AudioWorkletBridge.ts
    - SharedBuffer.ts
  /worker/
    - RenderWorker.ts
    - Messages.ts
  /renderers/
    - WebGPURenderer.ts
    - WebGLRenderer.ts
    - CanvasRenderer.ts
  /visualizers/
    - BaseVisualizer.ts
    - presets/*
  /utils/
    - Telemetry.ts
    - Color.ts
    - Easing.ts

/components/
  - VisualizationCanvas.tsx
  - VisualizationControls.tsx
  - VisualizationPicker.tsx
```

Implementation phases:
- Phase 1: Canvas2D + WebGL2 worker loop, AnalyserNode bridge, basic presets.
- Phase 2: AudioWorklet + SharedArrayBuffer, adaptive quality scaler, telemetry.
- Phase 3: WebGPU renderer with compute-driven effects, advanced presets.

I support Gemini's worker-centric approach, tempered with Claude's modular layering and explicit fallbacks. This balances reach, performance, and maintainability.

---

### ðŸŸ£ Claude - Round 2
**Timestamp:** 2025-12-04 13:36:45

#### Analysis of Previous Discussion

Gemini pushed hard on worker-based rendering and WebGPU-first, citing performance isolation and modern GPU capabilities. GPT synthesized both positions into a phased, tiered approach with capability detection and progressive enhancement. Both have moved me significantly from my initial conservative stance.

**Key insights I'm absorbing:**
1. **Threading is non-negotiable**: Both colleagues correctly identified that main-thread rendering in a Next.js app will cause frame drops during UI interactions. OffscreenCanvas + Worker is the right call.
2. **WebGPU timing**: Gemini's "WebGL2 is legacy" is technically prematureâ€”browser support is still incomplete in 2025â€”but the *direction* is correct. We should architect for WebGPU while maintaining practical fallbacks.
3. **SharedArrayBuffer**: This is the elegant solution for audio data transfer I missed. Zero-copy semantics are crucial for 60fps.
4. **Phased rollout**: GPT's staged implementation plan is pragmatic and de-risks the architecture.

#### My Thinking

<think>
Where I initially erred: I underestimated the main thread contention in a modern React app. My "optimize if profiling shows issues" stance was reactive rather than proactive. The worker-centric approach should be the baseline, not an optimization.

However, I want to push back gently on one aspect: **complexity budget**. We're adding:
- Worker lifecycle management
- SharedArrayBuffer coordination
- AudioWorklet fallback logic
- Three renderer implementations
- Capability detection matrix

This is substantial. We need clear documentation, error boundaries, and developer tooling. The architecture must include:
- **Error recovery**: If the worker crashes, gracefully disable visualization without breaking playback
- **Development mode**: Allow main-thread rendering in dev for easier debugging
- **Testing strategy**: How do we test worker code? Mock SharedArrayBuffer?
- **Bundle size**: Workers add chunks. What's the cost?

I also want to elevate one concern: **Preset versioning** (GPT's question). If we ship presets that users can customize and save, we need a migration system when parameters change. This deserves explicit design.
</think>

#### Key Points

**Areas of Agreement:**
- âœ… Worker-centric rendering with OffscreenCanvas
- âœ… SharedArrayBuffer for audio data transfer
- âœ… Tiered capability detection (WebGPU â†’ WebGL2 â†’ Canvas2D)
- âœ… AudioWorklet as primary analysis path
- âœ… Plugin architecture for visualizers
- âœ… Phased implementation approach

**Refinements I propose:**

1. **Error Boundaries & Graceful Degradation:**
   ```
   /lib/visualization/
     /core/
       - ErrorRecovery.ts
       - FallbackManager.ts
   ```
   - If worker fails to initialize â†’ fallback to main thread Canvas2D
   - If SharedArrayBuffer unavailable (security context) â†’ use postMessage with throttling
   - If audio context fails CORS â†’ show static visualization or disable

2. **Development Experience:**
   ```
   /lib/visualization/
     /dev/
       - DevTools.tsx (Inspector overlay)
       - WorkerDebugProxy.ts (console.log bridging)
   ```
   - Dev mode flag to bypass worker for debugging
   - Performance inspector showing frame time, audio latency, memory usage
   - Preset editor with live preview

3. **Preset Schema & Versioning:**
   ```typescript
   interface VisualizerPreset {
     id: string;
     version: string; // semver
     visualizer: string;
     params: Record<string, unknown>;
     migrations?: Migration[]; // upgrade path
   }
   ```
   - Versioned presets with automatic migration
   - Schema validation at load time
   - Backward compatibility layer

4. **Testing Infrastructure:**
   - Mock OffscreenCanvas for unit tests
   - Headless browser tests for worker integration
   - Visual regression testing for presets
   - Audio fixture library for consistent testing

#### Questions for the Council

1. **Complexity vs. Capability Trade-off**: Are we over-engineering for v1? Should Phase 1 be even simpler (WebGL main-thread, defer worker until Phase 2)?

2. **Library Dependencies**: Should we use a thin abstraction layer (like `regl` for WebGL or `three.js` in minimal mode) to reduce boilerplate, or go fully custom for bundle size?

3. **Performance Budget**: What's our target bundle size for the core + one preset? 50KB? 100KB? This constrains our dependencies.

#### Position/Proposal

I **ACCEPT** the worker-centric, tiered-capability architecture with the following **refinements**:

**Final Unified Architecture:**
```
/lib/visualization/
  /capabilities/
    - detect.ts (WebGPU/WebGL/Canvas detection)
    - fallback.ts (degradation strategy)
  /bridge/
    - AudioWorkletBridge.ts (primary)
    - AnalyserNodeBridge.ts (fallback)
    - SharedBuffer.ts (ring buffer implementation)
  /worker/
    - RenderWorker.ts (game loop)
    - WorkerMessages.ts (type-safe messaging)
  /renderers/
    - BaseRenderer.ts (interface)
    - WebGPURenderer.ts
    - WebGLRenderer.ts
    - CanvasRenderer.ts
  /visualizers/
    - BaseVisualizer.ts (lifecycle interface)
    - PresetSchema.ts (validation & versioning)
    - /presets/
      - SpectrumBars.ts
      - Waveform.ts
      - Particles.ts
  /core/
    - VisualizationEngine.ts (orchestrator)
    - ErrorRecovery.ts (fallback logic)
  /utils/
    - Telemetry.ts
    - Color.ts
    - Easing.ts
  /dev/
    - DevTools.tsx (development inspector)

/components/
  - VisualizationCanvas.tsx (React wrapper)
  - VisualizationControls.tsx
  - VisualizationPicker.tsx
  - ErrorBoundary.tsx
```

**Implementation Phases (Refined):**
- **Phase 1 (MVP):** WebGL worker loop, AnalyserNode bridge, 3 presets, error recovery
- **Phase 2 (Performance):** AudioWorklet + SharedArrayBuffer, adaptive quality, telemetry
- **Phase 3 (Premium):** WebGPU renderer, compute shaders, advanced presets

**Testing Strategy:**
- Unit tests with mocked Canvas/Audio APIs
- Integration tests in headless Chrome
- Visual regression suite
- Performance benchmarks

**Non-Functional Requirements:**
- Core bundle: <75KB gzipped (excluding presets)
- 60fps on modern devices (2020+)
- 30fps graceful degradation on older devices
- <50ms audio-to-visual latency

I'm ready to move toward consensus. The architecture is converging well.

---

### ðŸ”µ Gemini - Round 2
**Timestamp:** 2025-12-04 13:40:00

#### Analysis of Previous Discussion
We have reached a strong convergence. Claude has graciously accepted the worker-centric model and added critical operational robustness (error boundaries, dev tools, versioning). GPT provided the necessary tiered structure to make my "WebGPU First" ambition practical for the real world.

#### My Thinking
<think>
1.  **Complexity Check**: Claude asks if we are over-engineering. I argue that *under-engineering* the threading model now will lead to a rewrite later. The worker architecture is the "hard part" that needs to be in the foundation. Adding it later is painful.
2.  **Dependencies**: To answer Claude's question about libraries: For <75KB budget, `three.js` is too heavy (usually ~150KB+ minified). We should use a lightweight abstraction. `regl` is great for WebGL but dead. `twgl.js` is tiny. For WebGPU, `wgpu-matrix` is essential. I propose we stick to **raw WebGL2/WebGPU with tiny math helpers**. This keeps us within the 75KB budget and ensures we own the performance loop.
3.  **Consensus**: The "Final Unified Architecture" proposed by Claude in Round 2 is excellent. It incorporates my performance demands, GPT's pragmatic phasing, and Claude's structural discipline.

</think>

#### Key Points
- **Dependency Decision**: Reject heavy engines (Three.js/Babylon) for the core. Use lightweight helpers (`twgl.js`, `wgpu-matrix`) to stay under the 75KB budget.
- **Complexity Justification**: The worker architecture is "Base Complexity" required for a high-performance media app. It is not over-engineering; it is *appropriate* engineering for the domain.
- **Phase 1 Adjustment**: I agree with the phased approach. Phase 1 (WebGL Worker + AnalyserNode) is the correct MVP. It proves the threading model without the complexity of AudioWorklets immediately.

#### Questions for the Council
None. I believe we have a complete architectural blueprint.

#### Position/Proposal
I **ACCEPT** Claude's "Final Unified Architecture" and the Refined Implementation Phases.

I specifically endorse:
1.  **Worker-Centric Core**: Non-negotiable for performance.
2.  **No Heavy Engines**: Custom renderers to maintain the <75KB budget.
3.  **Robust Error Handling**: Essential for the worker-based approach.

I am ready to propose consensus.

#### ðŸ CONSENSUS PROPOSAL
**Proposed by:** Gemini
**Proposal:** Adopt the **Unified Worker-Centric Tiered Architecture** as detailed in Claude's Round 2, with the specific constraint of using lightweight helpers (e.g., `twgl.js`, `wgpu-matrix`) instead of heavy 3D engines to meet the <75KB bundle budget.

Vote to accept:
- [x] Claude: âœ… ACCEPT
- [x] GPT: âœ… ACCEPT
- [x] Gemini: âœ… ACCEPT

---

### ðŸŸ¢ GPT - Round 2
**Timestamp:** 2025-12-04 13:42:20

#### Analysis of Previous Discussion
Geminiâ€™s Round 2 consolidates the unified architecture and proposes consensus with sensible constraints on dependencies and bundle budget. Claudeâ€™s refinements ensure operability, DX, and testing rigor. The plan is balanced and actionable.

#### My Thinking
<think>
The architecture satisfies performance (worker loop, SAB, Worklet), reach (tiered fallbacks), and maintainability (plugin visualizers, phased rollout). Dependency guidance aligns with our budget. Risks are addressed via error recovery and telemetry.
</think>

#### Key Points
- Accept worker-centric core with OffscreenCanvas and SharedArrayBuffer.
- Proceed with phased rollout; prioritize MVP with WebGL worker + AnalyserNode.
- Keep dependencies minimal; allow tiny math/helper libs only.
- Adopt preset schema + migrations and testing strategy outlined by Claude.

#### Position/Proposal
I **ACCEPT** Gemini's consensus proposal. Marking GPT as âœ… ACCEPT.

---

### ðŸŸ£ Claude - Round 3
**Timestamp:** 2025-12-04 13:44:00

#### Analysis of Previous Discussion

The council has converged beautifully. Gemini proposed formal consensus on the unified architecture, and GPT has accepted. Both colleagues have validated the synthesis of our collective thinking:

- Gemini's performance-first instincts (worker isolation, WebGPU trajectory)
- GPT's pragmatic phasing and capability detection
- My structural concerns (error recovery, DX tooling, testing, versioning)

#### My Thinking

<think>
This has been a productive deliberation. I started conservatively (main-thread, WebGL-only) and was rightfully challenged. The final architecture is superior to any individual proposal:

1. **Performance**: Worker-centric rendering eliminates UI contention
2. **Reach**: Tiered fallbacks ensure broad device support
3. **Maintainability**: Plugin architecture, phased rollout, clear testing strategy
4. **Operational excellence**: Error boundaries, telemetry, dev tools
5. **Future-proof**: WebGPU path without over-committing too early

The dependency constraint (lightweight helpers only, <75KB budget) is exactly right. It forces discipline while enabling practical abstractions.

This is consensus-worthy.
</think>

#### Key Points

**What we achieved together:**
- A complete, actionable architecture for a production-grade visualization engine
- Clear technical decisions on threading, rendering, audio analysis
- Practical constraints (bundle size, performance targets, phasing)
- Operational readiness (error handling, testing, DX tooling)

**The synthesis is stronger than the parts:**
- My layered modularity + Gemini's worker isolation + GPT's tiered fallbacks = robust system
- Performance without sacrificing compatibility
- Ambition (WebGPU) tempered by pragmatism (phased rollout)

#### Position/Proposal

I **ACCEPT** Gemini's consensus proposal without reservation.

The **Unified Worker-Centric Tiered Architecture** with:
- Worker-based rendering loop (OffscreenCanvas)
- SharedArrayBuffer for audio data transfer
- AudioWorklet primary, AnalyserNode fallback
- WebGPU â†’ WebGL2 â†’ Canvas2D capability tiers
- Lightweight helpers only (twgl.js, wgpu-matrix)
- <75KB bundle budget for core
- Phased implementation (Phase 1 MVP, Phase 2 Performance, Phase 3 Premium)
- Complete error recovery, testing, and DX tooling

Marking Claude as âœ… ACCEPT.

**Consensus is now UNANIMOUS.**

---

## ðŸŽ‰ Final Consensus

The AI Council has reached unanimous consensus on the visualization engine architecture for a Next.js-based music streaming platform.

**Adopted Architecture:** Unified Worker-Centric Tiered Architecture

**Core Decisions:**
1. **Threading Model:** Worker-based rendering with OffscreenCanvas (non-negotiable for performance isolation)
2. **Audio Bridge:** AudioWorklet + SharedArrayBuffer for zero-copy data transfer, with AnalyserNode fallback
3. **Rendering Tiers:** WebGPU (primary) â†’ WebGL2 (secondary) â†’ Canvas2D (minimal fallback)
4. **Dependencies:** Lightweight helpers only (twgl.js, wgpu-matrix); reject heavy engines (Three.js, Babylon.js)
5. **Bundle Budget:** <75KB gzipped for core (excluding presets)
6. **Plugin Architecture:** Dynamic imports, versioned preset schema with migrations
7. **Phased Rollout:**
   - Phase 1 (MVP): WebGL worker + AnalyserNode + 3 presets + error recovery
   - Phase 2 (Performance): AudioWorklet + SharedArrayBuffer + adaptive quality + telemetry
   - Phase 3 (Premium): WebGPU + compute shaders + advanced presets
8. **Operational Excellence:** Error boundaries, graceful degradation, dev tools, comprehensive testing
9. **Performance Targets:** 60fps on modern devices, 30fps graceful degradation, <50ms latency

**Session closed:** 2025-12-04 13:44:00

---